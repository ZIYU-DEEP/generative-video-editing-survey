\section{Introduction}
\paragraph{Text guided video editing.} The tentative category below follows from~\cite{wang2023gen}.
\begin{itemize}
    \item \textbf{Zero-shot tuning with pre-trained T2V model}: Dreamix~\citep{molad2023dreamix}; ~\cite{ge2022long} jointly use time-agnostic VQ-GAN and time-senstive transformer; 
    \item \textbf{Zero-shot tuning with pre-trained T2I model}: Text2Video-Zero~\citep{khachatryan2023text2video}; Video-P2P~\citep{liu2023video}; Fatezero~\citep{qi2023fatezero}; Vid2VideZero~\citep{wang2023zero}; Pix2Video~\citep{ceylan2023pix2video}; \citep{lee2023shape}; VideoEdit\citep{couairon2023videdit}; 
    \item \textbf{One-shot tuning with pre-trained T2I model}: Tune-A-Video~\citep{wu2022tune}; VideoLDM~\citep{blattmann2023align}; SinFusion~\citep{nikankin2022sinfusion}; InstructVid2Vid~\citep{qin2023instructvid2vid}; 

\end{itemize}

\paragraph{Misc.} Here to summarize some other papers, like motion guided (videocomposer), stroke/flow guided, speech guided.


One major challenge of generative video editing is to maintain temporal consistency. The mainstream of this line of study uses attention-based approach to control temporal consistency~\citep{wu2022tune}. Recently, there is a new line of work using other inductive biases to for consistency control, \eg Rerender-A-Video~\citep{yang2023rerender} uses optical flow, \cite{lee2023shape} uses dense semantic correspondence to 
